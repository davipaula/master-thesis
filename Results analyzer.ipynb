{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook allows to explore the results of predicting the `click_rate` from `source_article` to `target_article` using different models (Doc2Vec, Wikipedia2Vec, Smash-RNN Paragraph Level, Smash-RNN Sentence Level and Smash-RNN Word Level).\n",
    "\n",
    "The class `ResultsAnalyzer` encapsules the logic to compute the results. Main features:\n",
    "- `get_ndcg_for_all_models`: Calculates the Normalized Discounted Cumulative Gain for each model\n",
    "- `get_map_for_all_models`: Calculates the Mean Average Precision for each model\n",
    "- `get_top_5_predicted_by_article_and_model(source_article, model)`: Gets the top 5 predictions for the `source_article`. The column `is_in_top_5` shows if the `target_article` is in the **actual** top 5 click rate.\n",
    "- `ResultsAnalyzer.results`: It is a Pandas Datafram containing the consolidated results\n",
    "- `get_sample_source_articles`: Samples 10 random `source_articles`. Can be used to manually check the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from results_analyzer import ResultsAnalyzer\n",
    "\n",
    "results_analyzer = ResultsAnalyzer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting NDCG for all models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2020-09-29 16:37:49,948] [INFO] Getting features from DB (calculate_statistics_per_group@results_analyzer.py:342)\n",
      "[2020-09-29 16:37:50,280] [INFO] Getting predictions by model (calculate_statistics_per_group@results_analyzer.py:358)\n",
      "[2020-09-29 16:37:50,300] [INFO] Aggregating predictions for each model (get_predictions_by_model@results_analyzer.py:266)\n",
      "100%|██████████| 16/16 [00:10<00:00,  1.59it/s]\n",
      "[2020-09-29 16:38:00,434] [INFO] Calculating results by model (calculate_statistics_per_group@results_analyzer.py:399)\n",
      "100%|██████████| 474/474 [00:04<00:00, 105.79it/s]\n",
      "100%|██████████| 474/474 [00:04<00:00, 106.28it/s]\n",
      "100%|██████████| 474/474 [00:04<00:00, 107.04it/s]\n",
      "100%|██████████| 474/474 [00:04<00:00, 107.32it/s]\n"
     ]
    }
   ],
   "source": [
    "results = results_analyzer.calculate_statistics_per_group()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source_article</th>\n",
       "      <th>paragraph_level_200d_concat_vfinal_introduction_only</th>\n",
       "      <th>sentence_level_200d_concat_vfinal_introduction_only</th>\n",
       "      <th>word_level_200d_concat_vfinal_introduction_only</th>\n",
       "      <th>word_level_50d_introduction_only</th>\n",
       "      <th>word_count</th>\n",
       "      <th>out_links_count</th>\n",
       "      <th>in_links_count</th>\n",
       "      <th>paragraph_count</th>\n",
       "      <th>sentence_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>.45 ACP</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2936.0</td>\n",
       "      <td>89.0</td>\n",
       "      <td>452.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>114.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1976 United States presidential election</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3671.0</td>\n",
       "      <td>254.0</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>134.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1992 NCAA Division I Men's Basketball Tournament</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>610.0</td>\n",
       "      <td>225.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>25.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1997 NCAA Division I Men's Basketball Tournament</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>627.0</td>\n",
       "      <td>205.0</td>\n",
       "      <td>69.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>24.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1999 NBA Finals</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1782.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>51.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>469</th>\n",
       "      <td>Woman</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6018.0</td>\n",
       "      <td>297.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>63.0</td>\n",
       "      <td>232.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>470</th>\n",
       "      <td>World's fair</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5407.0</td>\n",
       "      <td>397.0</td>\n",
       "      <td>220.0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>159.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>471</th>\n",
       "      <td>Wyatt Russell</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>348.0</td>\n",
       "      <td>58.0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>472</th>\n",
       "      <td>Zoë Kravitz</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1444.0</td>\n",
       "      <td>130.0</td>\n",
       "      <td>151.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>26.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>473</th>\n",
       "      <td>Zyzzyx Road</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>599.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>20.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>474 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       source_article  \\\n",
       "0                                             .45 ACP   \n",
       "1            1976 United States presidential election   \n",
       "2    1992 NCAA Division I Men's Basketball Tournament   \n",
       "3    1997 NCAA Division I Men's Basketball Tournament   \n",
       "4                                     1999 NBA Finals   \n",
       "..                                                ...   \n",
       "469                                             Woman   \n",
       "470                                      World's fair   \n",
       "471                                     Wyatt Russell   \n",
       "472                                       Zoë Kravitz   \n",
       "473                                       Zyzzyx Road   \n",
       "\n",
       "     paragraph_level_200d_concat_vfinal_introduction_only  \\\n",
       "0                                                  0.0      \n",
       "1                                                  0.0      \n",
       "2                                                  0.0      \n",
       "3                                                  0.0      \n",
       "4                                                  0.0      \n",
       "..                                                 ...      \n",
       "469                                                0.0      \n",
       "470                                                0.0      \n",
       "471                                                0.0      \n",
       "472                                                0.0      \n",
       "473                                                0.0      \n",
       "\n",
       "     sentence_level_200d_concat_vfinal_introduction_only  \\\n",
       "0                                                  0.0     \n",
       "1                                                  0.0     \n",
       "2                                                  0.0     \n",
       "3                                                  0.0     \n",
       "4                                                  0.0     \n",
       "..                                                 ...     \n",
       "469                                                0.0     \n",
       "470                                                0.0     \n",
       "471                                                0.0     \n",
       "472                                                0.0     \n",
       "473                                                0.0     \n",
       "\n",
       "     word_level_200d_concat_vfinal_introduction_only  \\\n",
       "0                                                0.0   \n",
       "1                                                0.0   \n",
       "2                                                0.0   \n",
       "3                                                0.0   \n",
       "4                                                0.0   \n",
       "..                                               ...   \n",
       "469                                              0.0   \n",
       "470                                              0.0   \n",
       "471                                              0.0   \n",
       "472                                              0.0   \n",
       "473                                              0.0   \n",
       "\n",
       "     word_level_50d_introduction_only  word_count  out_links_count  \\\n",
       "0                                 0.0      2936.0             89.0   \n",
       "1                                 0.0      3671.0            254.0   \n",
       "2                                 0.0       610.0            225.0   \n",
       "3                                 0.0       627.0            205.0   \n",
       "4                                 0.0      1782.0             60.0   \n",
       "..                                ...         ...              ...   \n",
       "469                               0.0      6018.0            297.0   \n",
       "470                               0.0      5407.0            397.0   \n",
       "471                               0.0       348.0             58.0   \n",
       "472                               0.0      1444.0            130.0   \n",
       "473                               0.0       599.0             25.0   \n",
       "\n",
       "     in_links_count  paragraph_count  sentence_count  \n",
       "0             452.0             37.0           114.0  \n",
       "1            1326.0             39.0           134.0  \n",
       "2              73.0              8.0            25.0  \n",
       "3              69.0             12.0            24.0  \n",
       "4              90.0             35.0            51.0  \n",
       "..              ...              ...             ...  \n",
       "469           150.0             63.0           232.0  \n",
       "470           220.0             52.0           159.0  \n",
       "471            52.0              8.0             9.0  \n",
       "472           151.0             23.0            26.0  \n",
       "473            11.0              9.0            20.0  \n",
       "\n",
       "[474 rows x 10 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models = [\"word_level_50d_concat_v2_introduction_only\",\n",
    "          \"word_level_50d_concat_v3_introduction_only\",\n",
    "         \"word_level_50d_concat_v4_introduction_only\",\n",
    "         \"word_level_200d_concat_vfinal_introduction_only\"]\n",
    "\n",
    "results\n",
    "\n",
    "# results[models].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "WORD_COUNT_BIN = \"word_count_bin\"\n",
    "WORD_COUNT_COLUMN = \"word_count\"\n",
    "OUT_LINKS_BIN = \"out_links_bin\"\n",
    "OUT_LINKS_COLUMN= \"out_links_count\"\n",
    "IN_LINKS_BIN = \"in_links_bin\"\n",
    "IN_LINKS_COLUMN = \"in_links_count_column\"\n",
    "PARAGRAPH_COUNT_COLUMN = \"paragraph_count\"\n",
    "PARAGRAPH_COUNT_BIN = \"paragraph_count_bin\"\n",
    "SENTENCE_COUNT_COLUMN = \"sentence_count\"\n",
    "SENTENCE_COUNT_BIN = \"sentence_count_bin\"\n",
    "MODEL_COLUMN = \"model\"\n",
    "\n",
    "ALL_FEATURES = [WORD_COUNT_COLUMN, OUT_LINKS_COLUMN, IN_LINKS_COLUMN]\n",
    "\n",
    "selected_models = [\n",
    "            \"doc2vec_no_sigmoid\",\n",
    "            \"wikipedia2vec_no_sigmoid\",\n",
    "            \"word_no_sigmoid\",\n",
    "            \"sentence_no_sigmoid\",\n",
    "            \"paragraph_no_sigmoid\",\n",
    "        ]\n",
    "\n",
    "clean_model_names = {\n",
    "    \"doc2vec_no_sigmoid\": \"Doc2Vec\",\n",
    "    \"paragraph_no_sigmoid\": \"SMASH RNN (P + S + W)\",\n",
    "    \"sentence_no_sigmoid\": \"SMASH RNN (P + S)\",\n",
    "    \"wikipedia2vec_no_sigmoid\": \"Wikipedia2Vec\",\n",
    "    \"word_no_sigmoid\": \"SMASH RNN (P)\",\n",
    "}\n",
    "\n",
    "SMASH_HATCH = '//'\n",
    "DOC2VEC_HATCH = '' \n",
    "WIKIPEDIA2VEC_HATCH = ''\n",
    "\n",
    "system_styles = {\n",
    "    'doc2vec_no_sigmoid': dict(color='lightcoral', hatch=DOC2VEC_HATCH),\n",
    "        \n",
    "    'wikipedia2vec_no_sigmoid': dict(color='yellow', hatch=WIKIPEDIA2VEC_HATCH),\n",
    "    \n",
    "    'paragraph_no_sigmoid': dict(color='blue', hatch=SMASH_HATCH),\n",
    "    'sentence_no_sigmoid': dict(color='green', hatch=SMASH_HATCH),\n",
    "    'word_no_sigmoid': dict(color='red', hatch=SMASH_HATCH),\n",
    "}\n",
    "\n",
    "SMALL_SIZE = 10\n",
    "MEDIUM_SIZE = 12\n",
    "BIGGER_SIZE = 14\n",
    "\n",
    "plt.rc('font', size=SMALL_SIZE)          # controls default text sizes\n",
    "plt.rc('axes', titlesize=SMALL_SIZE)     # fontsize of the axes title\n",
    "plt.rc('axes', labelsize=MEDIUM_SIZE)    # fontsize of the x and y labels\n",
    "plt.rc('xtick', labelsize=SMALL_SIZE+1)    # fontsize of the tick labels\n",
    "plt.rc('ytick', labelsize=SMALL_SIZE+1)    # fontsize of the tick labels\n",
    "plt.rc('legend', fontsize=SMALL_SIZE+1)    # legend fontsize\n",
    "plt.rc('figure', titlesize=BIGGER_SIZE)  # fontsize of the figure title\n",
    "\n",
    "plt.rc('pdf', fonttype=42)\n",
    "plt.rc('ps', fonttype=42)\n",
    "\n",
    "plt.rc('text', usetex=False)\n",
    "plt.rc('font', family='serif')\n",
    "\n",
    "def get_performance_figure(\n",
    "    results,\n",
    "    models,\n",
    "    feature_column,\n",
    "    x_label,\n",
    "    y_label=None,\n",
    "    figsize=(13, 6),\n",
    "    legend_columns_count=3,\n",
    "    buckets_count=5,\n",
    "    save_file_path=None,\n",
    "):\n",
    "    bin_column = f\"{feature_column}_bin\"\n",
    "    bins = pd.qcut(results[feature_column], q=buckets_count)\n",
    "\n",
    "    results[bin_column] = bins\n",
    "    result_by_model = results.groupby([bin_column]).mean()[models]\n",
    "\n",
    "    fig = plt.figure(figsize=figsize)\n",
    "\n",
    "    ax = result_by_model.plot(\n",
    "        kind=\"bar\", ax=fig.gca(), rot=0, width=0.7, alpha=0.9, edgecolor=[\"black\"],\n",
    "    )\n",
    "\n",
    "    box = ax.get_position()\n",
    "    ax.set_position([box.x0, box.y0 + box.height * 0.25, box.width, box.height * 0.75])\n",
    "\n",
    "    # Formats the bars\n",
    "    for container in ax.containers:\n",
    "        container_system = container.get_label()\n",
    "        \n",
    "        style = system_styles[container_system]\n",
    "        for patch in container.patches:\n",
    "            if 'color' in style:\n",
    "                patch.set_color(style['color'])\n",
    "            if 'hatch' in style:\n",
    "                patch.set_hatch(style['hatch'])\n",
    "            if 'linewidth' in style:\n",
    "                patch.set_linewidth(style['linewidth'])\n",
    "            if 'edgecolor' in style:\n",
    "                patch.set_edgecolor(style['edgecolor'])\n",
    "            else:\n",
    "                patch.set_edgecolor('black')\n",
    "\n",
    "    \n",
    "    model_names = [clean_model_names[model] for model in selected_models]\n",
    "\n",
    "    ax.legend(\n",
    "        model_names,\n",
    "        ncol=legend_columns_count,\n",
    "        loc=\"upper center\",\n",
    "        fancybox=True,\n",
    "        shadow=False,\n",
    "        bbox_to_anchor=(0.5, 1.2),\n",
    "    )\n",
    "\n",
    "    # Formats the x label as \"(lower, upper]\"\n",
    "    ax.set_xticklabels(\n",
    "        [f\"({int(i.left)}, {int(i.right)}]\" for i in bins.cat.categories]\n",
    "    )\n",
    "\n",
    "    y_label = \"NDCG@k (k=5)\"\n",
    "    ax.set_xlabel(x_label % len(result_by_model))\n",
    "    ax.set_ylabel(y_label)\n",
    "    \n",
    "    if save_file_path:\n",
    "        pdf_dpi = 300\n",
    "\n",
    "        logger.info(f\"Saved to {save_file_path}\")\n",
    "        plt.savefig(save_file_path, bbox_inches=\"tight\", dpi=pdf_dpi)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_performance_figure(results, selected_models, WORD_COUNT_COLUMN, \"Text length as word count (%s equal-sized buckets)\")\n",
    "get_performance_figure(results, selected_models, SENTENCE_COUNT_COLUMN, \"NNText length as word count (%s equal-sized buckets)\")\n",
    "get_performance_figure(results, selected_models, PARAGRAPH_COUNT_COLUMN, \"NMNText length as word count (%s equal-sized buckets)\")\n",
    "\n",
    "\n",
    "# results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\"wikipedia2vec_no_sigmoid\", \"word_no_sigmoid\"]\n",
    "get_performance_figure(ndcg_by_model_and_article, models, WORD_COUNT_COLUMN, 'Text length as word count (%s equal-sized buckets)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_analyzer.get_map_for_all_models()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting a sample of the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_analyzer.results.sample(n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting a sample of the source articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "results_analyzer.get_sample_source_articles()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting all the available models (models `paragraph`, `sentence` and `word` refer to Smash-RNN levels.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_analyzer.get_models()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting the top 5 predictions for a `source_article` and a `model`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_source_article = \"Quantum mechanics\"\n",
    "model = \"wikipedia2vec\"\n",
    "\n",
    "results_analyzer.get_top_10_predicted_by_article_and_model(sample_source_article)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next steps:\n",
    "- Create some analytics to understand better the results for each model (I will need help here!)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.4 64-bit ('thesis-davi': conda)",
   "language": "python",
   "name": "python37464bitthesisdaviconda173753154baf4ea5aebc1bb5b4a5349a"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
