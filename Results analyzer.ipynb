{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook allows to explore the results of predicting the `click_rate` from `source_article` to `target_article` using different models (Doc2Vec, Wikipedia2Vec, Smash-RNN Paragraph Level, Smash-RNN Sentence Level and Smash-RNN Word Level).\n",
    "\n",
    "The class `ResultsAnalyzer` encapsules the logic to compute the results. Main features:\n",
    "- `get_ndcg_for_all_models`: Calculates the Normalized Discounted Cumulative Gain for each model\n",
    "- `get_map_for_all_models`: Calculates the Mean Average Precision for each model\n",
    "- `get_top_5_predicted_by_article_and_model(source_article, model)`: Gets the top 5 predictions for the `source_article`. The column `is_in_top_5` shows if the `target_article` is in the **actual** top 5 click rate.\n",
    "- `ResultsAnalyzer.results`: It is a Pandas Datafram containing the consolidated results\n",
    "- `get_sample_source_articles`: Samples 10 random `source_articles`. Can be used to manually check the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "In /Users/dnascimentodepau/anaconda3/envs/thesis-davi/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle: \n",
      "The text.latex.preview rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "In /Users/dnascimentodepau/anaconda3/envs/thesis-davi/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle: \n",
      "The mathtext.fallback_to_cm rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "In /Users/dnascimentodepau/anaconda3/envs/thesis-davi/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle: Support for setting the 'mathtext.fallback_to_cm' rcParam is deprecated since 3.3 and will be removed two minor releases later; use 'mathtext.fallback : 'cm' instead.\n",
      "In /Users/dnascimentodepau/anaconda3/envs/thesis-davi/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle: \n",
      "The validate_bool_maybe_none function was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "In /Users/dnascimentodepau/anaconda3/envs/thesis-davi/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle: \n",
      "The savefig.jpeg_quality rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "In /Users/dnascimentodepau/anaconda3/envs/thesis-davi/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle: \n",
      "The keymap.all_axes rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "In /Users/dnascimentodepau/anaconda3/envs/thesis-davi/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle: \n",
      "The animation.avconv_path rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "In /Users/dnascimentodepau/anaconda3/envs/thesis-davi/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle: \n",
      "The animation.avconv_args rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n"
     ]
    }
   ],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "from results_analyzer import ResultsAnalyzer\n",
    "\n",
    "pd.options.display.float_format = '{:,.4f}'.format\n",
    "results_analyzer = ResultsAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "WORD_COUNT_BIN = \"word_count_bin\"\n",
    "WORD_COUNT_COLUMN = \"word_count\"\n",
    "OUT_LINKS_BIN = \"out_links_bin\"\n",
    "OUT_LINKS_COLUMN= \"out_links_count\"\n",
    "IN_LINKS_BIN = \"in_links_bin\"\n",
    "IN_LINKS_COLUMN = \"in_links_count\"\n",
    "PARAGRAPH_COUNT_COLUMN = \"paragraph_count\"\n",
    "PARAGRAPH_COUNT_BIN = \"paragraph_count_bin\"\n",
    "SENTENCE_COUNT_COLUMN = \"sentence_count\"\n",
    "SENTENCE_COUNT_BIN = \"sentence_count_bin\"\n",
    "MISSING_WORDS_COLUMN = \"missing_words_percentage\"\n",
    "MISSING_WORDS_BIN = \"missing_words_percentage_bin\"\n",
    "MODEL_COLUMN = \"model\"\n",
    "\n",
    "ALL_FEATURES = [WORD_COUNT_COLUMN, OUT_LINKS_COLUMN, IN_LINKS_COLUMN]\n",
    "\n",
    "DOC2VEC_SIAMESE = \"doc2vec_siamese\"\n",
    "DOC2VEC_COSINE = \"doc2vec_cosine\"\n",
    "WIKIPEDIA2VEC_SIAMESE = \"wikipedia2vec_siamese\"\n",
    "WIKIPEDIA2VEC_COSINE = \"wikipedia2vec_cosine\"\n",
    "SMASH_WORD_LEVEL = \"smash_word_level\"\n",
    "SMASH_SENTENCE_LEVEL = \"smash_sentence_level\"\n",
    "SMASH_PARAGRAPH_LEVEL = \"smash_paragraph_level\"\n",
    "SMASH_WORD_LEVEL_INTRODUCTION = \"smash_word_level_introduction\"\n",
    "SMASH_SENTENCE_LEVEL_INTRODUCTION = \"smash_sentence_level_introduction\"\n",
    "SMASH_PARAGRAPH_LEVEL_INTRODUCTION = \"smash_paragraph_level_introduction\"\n",
    "\n",
    "ALL_MODELS = [DOC2VEC_SIAMESE, \n",
    "              DOC2VEC_COSINE, \n",
    "              WIKIPEDIA2VEC_SIAMESE, \n",
    "              WIKIPEDIA2VEC_COSINE, \n",
    "              SMASH_WORD_LEVEL, \n",
    "              SMASH_SENTENCE_LEVEL,\n",
    "              SMASH_PARAGRAPH_LEVEL, \n",
    "              SMASH_WORD_LEVEL_INTRODUCTION,\n",
    "              SMASH_SENTENCE_LEVEL_INTRODUCTION,\n",
    "              SMASH_PARAGRAPH_LEVEL_INTRODUCTION]\n",
    "\n",
    "INTRODUCTION_MODELS = [SMASH_WORD_LEVEL_INTRODUCTION,\n",
    "                       SMASH_SENTENCE_LEVEL_INTRODUCTION,\n",
    "                       SMASH_PARAGRAPH_LEVEL_INTRODUCTION]\n",
    "\n",
    "COMPLETE_MODELS = [DOC2VEC_SIAMESE, \n",
    "                   WIKIPEDIA2VEC_SIAMESE,  \n",
    "                   SMASH_WORD_LEVEL, \n",
    "                   SMASH_SENTENCE_LEVEL,\n",
    "                   SMASH_PARAGRAPH_LEVEL]\n",
    "\n",
    "COMPLETE_MODELS_SAVE_CONFIG = [\n",
    "    (PARAGRAPH_COUNT_COLUMN, \"Source article length as paragraph count (%s equal-sized buckets)\"),\n",
    "    (SENTENCE_COUNT_COLUMN, \"Source article length as sentence count (%s equal-sized buckets)\"),\n",
    "    (WORD_COUNT_COLUMN, \"Source article length as word count (%s equal-sized buckets)\"),\n",
    "    (OUT_LINKS_COLUMN, \"Number of links present in the source articles (%s equal-sized buckets)\"),\n",
    "    (IN_LINKS_COLUMN, \"Number of articles with links pointing to the source articles (%s equal-sized buckets)\"),\n",
    "    (MISSING_WORDS_COLUMN, \"Percentage of missing words in GloVe (%s equal-sized buckets)\")\n",
    "]\n",
    "\n",
    "SMASH_MODELS = [SMASH_WORD_LEVEL, \n",
    "                SMASH_SENTENCE_LEVEL,\n",
    "                SMASH_PARAGRAPH_LEVEL,]\n",
    "\n",
    "SMASH_AND_INTRODUCTION_MODELS = [SMASH_WORD_LEVEL, \n",
    "SMASH_SENTENCE_LEVEL,\n",
    "SMASH_PARAGRAPH_LEVEL,\n",
    "                                SMASH_WORD_LEVEL_INTRODUCTION,\n",
    "SMASH_SENTENCE_LEVEL_INTRODUCTION,\n",
    "SMASH_PARAGRAPH_LEVEL_INTRODUCTION]\n",
    "\n",
    "COSINE_MODELS = [DOC2VEC_SIAMESE, \n",
    "                 WIKIPEDIA2VEC_SIAMESE,\n",
    "                 DOC2VEC_COSINE, \n",
    "                 WIKIPEDIA2VEC_COSINE]\n",
    "\n",
    "BEST_MODELS = [DOC2VEC_SIAMESE,\n",
    "               SMASH_WORD_LEVEL,\n",
    "               WIKIPEDIA2VEC_SIAMESE]\n",
    "\n",
    "CLEAN_MODEL_NAMES = {\n",
    "    DOC2VEC_SIAMESE: \"Doc2Vec\",\n",
    "    DOC2VEC_COSINE: \"Doc2Vec Cosine\",\n",
    "    WIKIPEDIA2VEC_SIAMESE: \"Wikipedia2Vec\", \n",
    "    WIKIPEDIA2VEC_COSINE: \"Wikipedia2Vec Cosine\", \n",
    "    SMASH_WORD_LEVEL: \"SMASH RNN (P + S + W)\", \n",
    "    SMASH_SENTENCE_LEVEL: \"SMASH RNN (P + S)\",\n",
    "    SMASH_PARAGRAPH_LEVEL: \"SMASH RNN (P)\", \n",
    "    SMASH_WORD_LEVEL_INTRODUCTION: \"SMASH RNN Introduction (P + S + W)\",\n",
    "    SMASH_SENTENCE_LEVEL_INTRODUCTION: \"SMASH RNN Introduction (P + S)\",\n",
    "    SMASH_PARAGRAPH_LEVEL_INTRODUCTION: \"SMASH RNN Introduction (P)\",\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting NDCG for all models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2020-11-13 17:27:50,458] [INFO] Getting features from DB (calculate_statistics_per_article@results_analyzer.py:407)\n",
      "[2020-11-13 17:28:02,456] [INFO] Getting predictions by model (calculate_statistics_per_article@results_analyzer.py:426)\n",
      "[2020-11-13 17:28:02,466] [INFO] Aggregating predictions for each model (get_predictions_by_model@results_analyzer.py:246)\n",
      "100%|██████████| 10/10 [00:07<00:00,  1.39it/s]\n",
      "[2020-11-13 17:28:09,687] [INFO] Calculating results by model (calculate_statistics_per_article@results_analyzer.py:435)\n",
      "100%|██████████| 474/474 [00:04<00:00, 115.30it/s]\n",
      "100%|██████████| 474/474 [00:04<00:00, 115.33it/s]\n",
      "100%|██████████| 474/474 [00:03<00:00, 119.98it/s]\n",
      "100%|██████████| 474/474 [00:03<00:00, 121.29it/s]\n",
      "100%|██████████| 474/474 [00:03<00:00, 119.25it/s]\n",
      "100%|██████████| 474/474 [00:03<00:00, 119.17it/s]\n",
      "100%|██████████| 474/474 [00:03<00:00, 121.22it/s]\n",
      "100%|██████████| 474/474 [00:03<00:00, 122.80it/s]\n",
      "100%|██████████| 474/474 [00:04<00:00, 118.15it/s]\n",
      "100%|██████████| 474/474 [00:04<00:00, 117.68it/s]\n"
     ]
    }
   ],
   "source": [
    "_results = results_analyzer.calculate_statistics_per_article()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_ks = results_analyzer.calculate_statistics_per_model_different_k(ks=[1, 3, 5, 10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(results_ks[results_ks[\"model\"].isin(COMPLETE_MODELS)].pivot(index=\"model\", columns=\"k\", values=\"map\").reset_index().to_latex(index=False))\n",
    "results_ks[results_ks[\"model\"].isin(COMPLETE_MODELS)].pivot(index=\"model\", columns=\"k\", values=\"ndcg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2020-11-09 18:51:33,497] [INFO] Aggregating predictions for each model (get_predictions_by_model@results_analyzer.py:246)\n",
      "100%|██████████| 10/10 [00:07<00:00,  1.36it/s]\n",
      "[2020-11-09 18:51:40,862] [INFO] Calculating results by model (calculate_statistics_per_model@results_analyzer.py:335)\n",
      "100%|██████████| 474/474 [00:04<00:00, 115.57it/s]\n",
      "100%|██████████| 474/474 [00:04<00:00, 98.44it/s] \n",
      "100%|██████████| 474/474 [00:04<00:00, 109.47it/s]\n",
      "100%|██████████| 474/474 [00:04<00:00, 99.06it/s] \n",
      "100%|██████████| 474/474 [00:04<00:00, 109.19it/s]\n",
      "100%|██████████| 474/474 [00:04<00:00, 110.26it/s]\n",
      "100%|██████████| 474/474 [00:04<00:00, 109.58it/s]\n",
      "100%|██████████| 474/474 [00:04<00:00, 106.17it/s]\n",
      "100%|██████████| 474/474 [00:04<00:00, 115.83it/s]\n",
      "100%|██████████| 474/474 [00:04<00:00, 109.66it/s]\n"
     ]
    }
   ],
   "source": [
    "results_per_model = results_analyzer.calculate_statistics_per_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                Model  NDCG@5  MAP@5  Precision@5\n",
      "2               smash_paragraph_level  0.4895 0.6305       0.4658\n",
      "3  smash_paragraph_level_introduction  0.4724 0.6020       0.4570\n",
      "4                smash_sentence_level  0.4769 0.6270       0.4492\n",
      "5   smash_sentence_level_introduction  0.4728 0.6055       0.4570\n",
      "6                    smash_word_level  0.4972 0.6416       0.4700\n",
      "7       smash_word_level_introduction  0.4818 0.6217       0.4593\n"
     ]
    }
   ],
   "source": [
    "def get_clean_results(model_results, selected_models = COMPLETE_MODELS):\n",
    "    clean_results = model_results[model_results[\"model\"].isin(selected_models)].copy()\n",
    "    \n",
    "    clean_results.columns = [\"Model\", \"NDCG@5\", \"MAP@5\", \"Precision@5\"]\n",
    "    \n",
    "    return clean_results\n",
    "\n",
    "res = get_clean_results(results_per_model, SMASH_AND_INTRODUCTION_MODELS)\n",
    "\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source_article</th>\n",
       "      <th>wikipedia2vec_siamese</th>\n",
       "      <th>smash_word_level</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>History of Japan</td>\n",
       "      <td>0.2318</td>\n",
       "      <td>0.0734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>Gary Oldman</td>\n",
       "      <td>0.2201</td>\n",
       "      <td>0.1799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>271</th>\n",
       "      <td>List of years in home video</td>\n",
       "      <td>0.2394</td>\n",
       "      <td>0.0663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>247</th>\n",
       "      <td>List of The Vampire Diaries characters</td>\n",
       "      <td>0.1389</td>\n",
       "      <td>0.1478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>Eton College</td>\n",
       "      <td>0.1635</td>\n",
       "      <td>0.1389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>333</th>\n",
       "      <td>Pol Pot</td>\n",
       "      <td>0.3786</td>\n",
       "      <td>0.1370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>461</th>\n",
       "      <td>Warship</td>\n",
       "      <td>0.4011</td>\n",
       "      <td>0.1546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>464</th>\n",
       "      <td>West Bromwich Albion F.C.</td>\n",
       "      <td>0.3024</td>\n",
       "      <td>0.0694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>206</th>\n",
       "      <td>Jeremy Clarkson</td>\n",
       "      <td>0.4441</td>\n",
       "      <td>0.1100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>292</th>\n",
       "      <td>Michael Biehn</td>\n",
       "      <td>0.4074</td>\n",
       "      <td>0.1682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>24 Hours of Le Mans</td>\n",
       "      <td>0.0663</td>\n",
       "      <td>0.1478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>241</th>\n",
       "      <td>List of DreamWorks Animation productions</td>\n",
       "      <td>0.1389</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>Celebrity Rehab with Dr. Drew</td>\n",
       "      <td>0.2240</td>\n",
       "      <td>0.1488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>312</th>\n",
       "      <td>New World Order (conspiracy theory)</td>\n",
       "      <td>0.3120</td>\n",
       "      <td>0.1100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>342</th>\n",
       "      <td>RadhaKrishn</td>\n",
       "      <td>0.3281</td>\n",
       "      <td>0.1642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>256</th>\n",
       "      <td>List of large aircraft</td>\n",
       "      <td>0.2376</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>326</th>\n",
       "      <td>Patrick Dempsey</td>\n",
       "      <td>0.3301</td>\n",
       "      <td>0.0784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>453</th>\n",
       "      <td>Vietnam War</td>\n",
       "      <td>0.2337</td>\n",
       "      <td>0.0694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>266</th>\n",
       "      <td>List of supporting Harry Potter characters</td>\n",
       "      <td>0.4323</td>\n",
       "      <td>0.1447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>393</th>\n",
       "      <td>Subhash Ghai</td>\n",
       "      <td>0.1610</td>\n",
       "      <td>0.1478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249</th>\n",
       "      <td>List of adult television channels</td>\n",
       "      <td>0.1737</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>211</th>\n",
       "      <td>John Browning</td>\n",
       "      <td>0.3836</td>\n",
       "      <td>0.1389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>Anton Chekhov</td>\n",
       "      <td>0.7126</td>\n",
       "      <td>0.1993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>239</th>\n",
       "      <td>List of Batman family enemies</td>\n",
       "      <td>0.2863</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>.45 ACP</td>\n",
       "      <td>0.4519</td>\n",
       "      <td>0.1795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2020 coronavirus pandemic in New York (state)</td>\n",
       "      <td>0.2337</td>\n",
       "      <td>0.1584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>414</th>\n",
       "      <td>The Irishman</td>\n",
       "      <td>0.3882</td>\n",
       "      <td>0.1389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>458</th>\n",
       "      <td>WWE</td>\n",
       "      <td>0.3590</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>400</th>\n",
       "      <td>Sushmita Sen</td>\n",
       "      <td>0.0694</td>\n",
       "      <td>0.0734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2020 coronavirus pandemic in the United States</td>\n",
       "      <td>0.3052</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     source_article  wikipedia2vec_siamese  \\\n",
       "177                                History of Japan                 0.2318   \n",
       "152                                     Gary Oldman                 0.2201   \n",
       "271                     List of years in home video                 0.2394   \n",
       "247          List of The Vampire Diaries characters                 0.1389   \n",
       "132                                    Eton College                 0.1635   \n",
       "333                                         Pol Pot                 0.3786   \n",
       "461                                         Warship                 0.4011   \n",
       "464                       West Bromwich Albion F.C.                 0.3024   \n",
       "206                                 Jeremy Clarkson                 0.4441   \n",
       "292                                   Michael Biehn                 0.4074   \n",
       "23                              24 Hours of Le Mans                 0.0663   \n",
       "241        List of DreamWorks Animation productions                 0.1389   \n",
       "74                    Celebrity Rehab with Dr. Drew                 0.2240   \n",
       "312             New World Order (conspiracy theory)                 0.3120   \n",
       "342                                     RadhaKrishn                 0.3281   \n",
       "256                          List of large aircraft                 0.2376   \n",
       "326                                 Patrick Dempsey                 0.3301   \n",
       "453                                     Vietnam War                 0.2337   \n",
       "266      List of supporting Harry Potter characters                 0.4323   \n",
       "393                                    Subhash Ghai                 0.1610   \n",
       "249               List of adult television channels                 0.1737   \n",
       "211                                   John Browning                 0.3836   \n",
       "41                                    Anton Chekhov                 0.7126   \n",
       "239                   List of Batman family enemies                 0.2863   \n",
       "0                                           .45 ACP                 0.4519   \n",
       "18    2020 coronavirus pandemic in New York (state)                 0.2337   \n",
       "414                                    The Irishman                 0.3882   \n",
       "458                                             WWE                 0.3590   \n",
       "400                                    Sushmita Sen                 0.0694   \n",
       "20   2020 coronavirus pandemic in the United States                 0.3052   \n",
       "\n",
       "     smash_word_level  \n",
       "177            0.0734  \n",
       "152            0.1799  \n",
       "271            0.0663  \n",
       "247            0.1478  \n",
       "132            0.1389  \n",
       "333            0.1370  \n",
       "461            0.1546  \n",
       "464            0.0694  \n",
       "206            0.1100  \n",
       "292            0.1682  \n",
       "23             0.1478  \n",
       "241            0.0000  \n",
       "74             0.1488  \n",
       "312            0.1100  \n",
       "342            0.1642  \n",
       "256            0.0000  \n",
       "326            0.0784  \n",
       "453            0.0694  \n",
       "266            0.1447  \n",
       "393            0.1478  \n",
       "249            0.0000  \n",
       "211            0.1389  \n",
       "41             0.1993  \n",
       "239            0.0000  \n",
       "0              0.1795  \n",
       "18             0.1584  \n",
       "414            0.1389  \n",
       "458            0.0000  \n",
       "400            0.0734  \n",
       "20             0.0000  "
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_results[_results[\"smash_word_level\"] <= 0.2][[\"source_article\", \"wikipedia2vec_siamese\", \"smash_word_level\"]].sample(n=30)\n",
    "\n",
    "# Princess Victoria Louise of Prussia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>source_article</th>\n",
       "      <th>target_article</th>\n",
       "      <th>actual_click_rate</th>\n",
       "      <th>predicted_click_rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>17310</th>\n",
       "      <td>smash_word_level</td>\n",
       "      <td>Ruby Ridge</td>\n",
       "      <td>Rainbow Farm</td>\n",
       "      <td>0.0096</td>\n",
       "      <td>0.0189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3112</th>\n",
       "      <td>smash_word_level</td>\n",
       "      <td>Ghost Adventures</td>\n",
       "      <td>Echo Bridge Home Entertainment</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014</th>\n",
       "      <td>smash_word_level</td>\n",
       "      <td>2020 coronavirus pandemic in the United States</td>\n",
       "      <td>Ebola virus disease</td>\n",
       "      <td>0.0003</td>\n",
       "      <td>0.0085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2294</th>\n",
       "      <td>smash_word_level</td>\n",
       "      <td>First Mexican Empire</td>\n",
       "      <td>History of Belize (1506–1862)</td>\n",
       "      <td>0.0183</td>\n",
       "      <td>0.0079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4115</th>\n",
       "      <td>smash_word_level</td>\n",
       "      <td>The Spinners (American R&amp;B group)</td>\n",
       "      <td>Working My Way Back to You</td>\n",
       "      <td>0.0228</td>\n",
       "      <td>0.0111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2680</th>\n",
       "      <td>smash_word_level</td>\n",
       "      <td>Penicillin</td>\n",
       "      <td>University of Oxford</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>-0.0006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14869</th>\n",
       "      <td>smash_word_level</td>\n",
       "      <td>Tony Blair</td>\n",
       "      <td>Prime Minister's Questions</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10648</th>\n",
       "      <td>smash_word_level</td>\n",
       "      <td>List of cities in India by population</td>\n",
       "      <td>Pune</td>\n",
       "      <td>0.0146</td>\n",
       "      <td>0.0155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13891</th>\n",
       "      <td>smash_word_level</td>\n",
       "      <td>The Maze Runner (film)</td>\n",
       "      <td>Chris Sheffield</td>\n",
       "      <td>0.0072</td>\n",
       "      <td>0.0207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5102</th>\n",
       "      <td>smash_word_level</td>\n",
       "      <td>Anne Rice</td>\n",
       "      <td>Anne Rice bibliography</td>\n",
       "      <td>0.0222</td>\n",
       "      <td>0.0187</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  model                                  source_article  \\\n",
       "17310  smash_word_level                                      Ruby Ridge   \n",
       "3112   smash_word_level                                Ghost Adventures   \n",
       "2014   smash_word_level  2020 coronavirus pandemic in the United States   \n",
       "2294   smash_word_level                            First Mexican Empire   \n",
       "4115   smash_word_level               The Spinners (American R&B group)   \n",
       "2680   smash_word_level                                      Penicillin   \n",
       "14869  smash_word_level                                      Tony Blair   \n",
       "10648  smash_word_level           List of cities in India by population   \n",
       "13891  smash_word_level                          The Maze Runner (film)   \n",
       "5102   smash_word_level                                       Anne Rice   \n",
       "\n",
       "                       target_article  actual_click_rate  predicted_click_rate  \n",
       "17310                    Rainbow Farm             0.0096                0.0189  \n",
       "3112   Echo Bridge Home Entertainment             0.0000                0.0288  \n",
       "2014              Ebola virus disease             0.0003                0.0085  \n",
       "2294    History of Belize (1506–1862)             0.0183                0.0079  \n",
       "4115       Working My Way Back to You             0.0228                0.0111  \n",
       "2680             University of Oxford             0.0000               -0.0006  \n",
       "14869      Prime Minister's Questions             0.0000                0.0050  \n",
       "10648                            Pune             0.0146                0.0155  \n",
       "13891                 Chris Sheffield             0.0072                0.0207  \n",
       "5102           Anne Rice bibliography             0.0222                0.0187  "
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = pd.read_csv(\"./results/test/results_smash_word_level.csv\")\n",
    "a.sample(n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>source_article</th>\n",
       "      <th>target_article</th>\n",
       "      <th>actual_click_rate</th>\n",
       "      <th>predicted_click_rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>17147</th>\n",
       "      <td>smash_word_level</td>\n",
       "      <td>WWE</td>\n",
       "      <td>Capitol Wrestling Corporation</td>\n",
       "      <td>0.0088</td>\n",
       "      <td>0.0278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2833</th>\n",
       "      <td>smash_word_level</td>\n",
       "      <td>WWE</td>\n",
       "      <td>Glossary of professional wrestling terms</td>\n",
       "      <td>0.0012</td>\n",
       "      <td>0.0261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12381</th>\n",
       "      <td>smash_word_level</td>\n",
       "      <td>WWE</td>\n",
       "      <td>All Elite Wrestling</td>\n",
       "      <td>0.0021</td>\n",
       "      <td>0.0246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15723</th>\n",
       "      <td>smash_word_level</td>\n",
       "      <td>WWE</td>\n",
       "      <td>World Championship Wrestling</td>\n",
       "      <td>0.0029</td>\n",
       "      <td>0.0199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3497</th>\n",
       "      <td>smash_word_level</td>\n",
       "      <td>WWE</td>\n",
       "      <td>Under Armour</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3636</th>\n",
       "      <td>smash_word_level</td>\n",
       "      <td>WWE</td>\n",
       "      <td>WWE Studios</td>\n",
       "      <td>0.0075</td>\n",
       "      <td>0.0168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15014</th>\n",
       "      <td>smash_word_level</td>\n",
       "      <td>WWE</td>\n",
       "      <td>National Wrestling Alliance</td>\n",
       "      <td>0.0014</td>\n",
       "      <td>0.0167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4474</th>\n",
       "      <td>smash_word_level</td>\n",
       "      <td>WWE</td>\n",
       "      <td>Professional wrestling promotion</td>\n",
       "      <td>0.0026</td>\n",
       "      <td>0.0161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>675</th>\n",
       "      <td>smash_word_level</td>\n",
       "      <td>WWE</td>\n",
       "      <td>Syfy</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13021</th>\n",
       "      <td>smash_word_level</td>\n",
       "      <td>WWE</td>\n",
       "      <td>Tapout (clothing brand)</td>\n",
       "      <td>0.0084</td>\n",
       "      <td>0.0156</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  model source_article  \\\n",
       "17147  smash_word_level            WWE   \n",
       "2833   smash_word_level            WWE   \n",
       "12381  smash_word_level            WWE   \n",
       "15723  smash_word_level            WWE   \n",
       "3497   smash_word_level            WWE   \n",
       "3636   smash_word_level            WWE   \n",
       "15014  smash_word_level            WWE   \n",
       "4474   smash_word_level            WWE   \n",
       "675    smash_word_level            WWE   \n",
       "13021  smash_word_level            WWE   \n",
       "\n",
       "                                 target_article  actual_click_rate  \\\n",
       "17147             Capitol Wrestling Corporation             0.0088   \n",
       "2833   Glossary of professional wrestling terms             0.0012   \n",
       "12381                       All Elite Wrestling             0.0021   \n",
       "15723              World Championship Wrestling             0.0029   \n",
       "3497                               Under Armour             0.0000   \n",
       "3636                                WWE Studios             0.0075   \n",
       "15014               National Wrestling Alliance             0.0014   \n",
       "4474           Professional wrestling promotion             0.0026   \n",
       "675                                        Syfy             0.0000   \n",
       "13021                   Tapout (clothing brand)             0.0084   \n",
       "\n",
       "       predicted_click_rate  \n",
       "17147                0.0278  \n",
       "2833                 0.0261  \n",
       "12381                0.0246  \n",
       "15723                0.0199  \n",
       "3497                 0.0190  \n",
       "3636                 0.0168  \n",
       "15014                0.0167  \n",
       "4474                 0.0161  \n",
       "675                  0.0160  \n",
       "13021                0.0156  "
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[a[\"source_article\"] == \"WWE\"].nlargest(10, 'predicted_click_rate')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article</th>\n",
       "      <th>text_ids</th>\n",
       "      <th>raw_text</th>\n",
       "      <th>text_ids_intro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Anarchism</td>\n",
       "      <td>[[[209, 926, 683], [580, 92, 802]], [[69, 682,...</td>\n",
       "      <td>[[['political', 'movement', 'form'], ['call', ...</td>\n",
       "      <td>[[[209, 926, 683], [580, 92, 802]], [[69, 682,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Autism</td>\n",
       "      <td>[[[528]], [[659, 62, 964, 964, 659]], [[488, 4...</td>\n",
       "      <td>[[['visit']], [['social', 'year', 'child', 'ch...</td>\n",
       "      <td>[[[528]], [[659, 62, 964, 964, 659]], [[488, 4...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Alabama</td>\n",
       "      <td>[[[92, 368, 104, 112], [718, 193, 322, 950, 85...</td>\n",
       "      <td>[[['state', 'region', 'united', 'states'], ['b...</td>\n",
       "      <td>[[[92, 368, 104, 112], [718, 193, 322, 950, 85...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Achilles</td>\n",
       "      <td>[[[136, 353, 323], [630, 691]], [[136, 587], [...</td>\n",
       "      <td>[[['war', 'great', 'central'], ['son', 'king']...</td>\n",
       "      <td>[[[136, 353, 323], [630, 691]], [[136, 587], [...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Abraham Lincoln</td>\n",
       "      <td>[[[617, 396, 140, 90, 104, 112], [410, 513, 35...</td>\n",
       "      <td>[[['february', 'april', 'american', 'president...</td>\n",
       "      <td>[[[617, 396, 140, 90, 104, 112], [410, 513, 35...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Aristotle</td>\n",
       "      <td>[[[566], [164], [580, 629, 556], [488, 403, 78...</td>\n",
       "      <td>[[['period'], ['school'], ['call', 'father', '...</td>\n",
       "      <td>[[[566], [164], [580, 629, 556], [488, 403, 78...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Academy Award for Best Production Design</td>\n",
       "      <td>[[[254, 618, 760, 319], [929, 254, 760, 511, 5...</td>\n",
       "      <td>[[['best', 'production', 'art', 'film'], ['ori...</td>\n",
       "      <td>[[[254, 618, 760, 319], [929, 254, 760, 511, 5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Academy Awards</td>\n",
       "      <td>[[[346, 319, 459], [454, 146, 538], [580], [76...</td>\n",
       "      <td>[[['know', 'film', 'industry'], ['given', 'int...</td>\n",
       "      <td>[[[346, 319, 459], [454, 146, 538], [580], [76...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>International Atomic Time</td>\n",
       "      <td>[[[146, 79, 348, 152, 79, 815, 79, 79], [79, 7...</td>\n",
       "      <td>[[['international', 'time', 'french', 'high', ...</td>\n",
       "      <td>[[[146, 79, 348, 152, 79, 815, 79, 79], [79, 7...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Altruism</td>\n",
       "      <td>[[[473, 134, 712, 214], [305]], [[348, 348, 89...</td>\n",
       "      <td>[[['human', 'being', 'result', 'life'], ['case...</td>\n",
       "      <td>[[[473, 134, 712, 214], [305]], [[348, 348, 89...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    article  \\\n",
       "0                                 Anarchism   \n",
       "1                                    Autism   \n",
       "2                                   Alabama   \n",
       "3                                  Achilles   \n",
       "4                           Abraham Lincoln   \n",
       "5                                 Aristotle   \n",
       "6  Academy Award for Best Production Design   \n",
       "7                            Academy Awards   \n",
       "8                 International Atomic Time   \n",
       "9                                  Altruism   \n",
       "\n",
       "                                            text_ids  \\\n",
       "0  [[[209, 926, 683], [580, 92, 802]], [[69, 682,...   \n",
       "1  [[[528]], [[659, 62, 964, 964, 659]], [[488, 4...   \n",
       "2  [[[92, 368, 104, 112], [718, 193, 322, 950, 85...   \n",
       "3  [[[136, 353, 323], [630, 691]], [[136, 587], [...   \n",
       "4  [[[617, 396, 140, 90, 104, 112], [410, 513, 35...   \n",
       "5  [[[566], [164], [580, 629, 556], [488, 403, 78...   \n",
       "6  [[[254, 618, 760, 319], [929, 254, 760, 511, 5...   \n",
       "7  [[[346, 319, 459], [454, 146, 538], [580], [76...   \n",
       "8  [[[146, 79, 348, 152, 79, 815, 79, 79], [79, 7...   \n",
       "9  [[[473, 134, 712, 214], [305]], [[348, 348, 89...   \n",
       "\n",
       "                                            raw_text  \\\n",
       "0  [[['political', 'movement', 'form'], ['call', ...   \n",
       "1  [[['visit']], [['social', 'year', 'child', 'ch...   \n",
       "2  [[['state', 'region', 'united', 'states'], ['b...   \n",
       "3  [[['war', 'great', 'central'], ['son', 'king']...   \n",
       "4  [[['february', 'april', 'american', 'president...   \n",
       "5  [[['period'], ['school'], ['call', 'father', '...   \n",
       "6  [[['best', 'production', 'art', 'film'], ['ori...   \n",
       "7  [[['know', 'film', 'industry'], ['given', 'int...   \n",
       "8  [[['international', 'time', 'french', 'high', ...   \n",
       "9  [[['human', 'being', 'result', 'life'], ['case...   \n",
       "\n",
       "                                      text_ids_intro  \n",
       "0  [[[209, 926, 683], [580, 92, 802]], [[69, 682,...  \n",
       "1  [[[528]], [[659, 62, 964, 964, 659]], [[488, 4...  \n",
       "2  [[[92, 368, 104, 112], [718, 193, 322, 950, 85...  \n",
       "3  [[[136, 353, 323], [630, 691]], [[136, 587], [...  \n",
       "4  [[[617, 396, 140, 90, 104, 112], [410, 513, 35...  \n",
       "5  [[[566], [164], [580, 629, 556], [488, 403, 78...  \n",
       "6  [[[254, 618, 760, 319], [929, 254, 760, 511, 5...  \n",
       "7  [[[346, 319, 459], [454, 146, 538], [580], [76...  \n",
       "8  [[[146, 79, 348, 152, 79, 815, 79, 79], [79, 7...  \n",
       "9  [[[473, 134, 712, 214], [305]], [[348, 348, 89...  "
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = pd.read_csv(\"./data/dataset/wiki_articles_english_complete_bkp_2.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"[[['list', 'island', 'europe', 'order', 'area']], [['data', 'island', 'island', 'russia']]]\"]\n"
     ]
    }
   ],
   "source": [
    "print(t[t[\"article\"] == \"List of European islands by area\"][\"raw_text\"].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{lll}\n",
      "\\toprule\n",
      "{} &                    actual click rate &               smash\\_word\\_level \\\\\n",
      "\\midrule\n",
      "0 &  List of 24 Hours of Le Mans winners &            24 Hours of Daytona \\\\\n",
      "1 &       Tom Kristensen (racing driver) &       1955 24 Hours of Le Mans \\\\\n",
      "2 &           Triple Crown of Motorsport &       1923 24 Hours of Le Mans \\\\\n",
      "3 &                 Circuit de la Sarthe &                   Grand tourer \\\\\n",
      "4 &             2019 24 Hours of Le Mans &             24 Hours of LeMons \\\\\n",
      "5 &                     Indianapolis 500 &       2019 24 Hours of Le Mans \\\\\n",
      "6 &                              Le Mans &       2010 24 Hours of Le Mans \\\\\n",
      "7 &                            Ford GT40 &               Indianapolis 500 \\\\\n",
      "8 &                         Joest Racing &           24 Hours Nürburgring \\\\\n",
      "9 &                              Porsche &  Endurance racing (motorsport) \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(results_analyzer.get_article_results(\"24 Hours of Le Mans\")[[\"actual click rate\", \"smash_word_level\"]].to_latex())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_latex(models):\n",
    "    clean_names = {k: v for k, v in CLEAN_MODEL_NAMES.items() if k in models}\n",
    "    models_results = _results[models].mean()\n",
    "    models_results.rename(index=clean_names, inplace=True)\n",
    "    \n",
    "    print(models_results.to_latex())\n",
    "    \n",
    "get_latex(SMASH_MODELS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "SMASH_HATCH = '//'\n",
    "DOC2VEC_HATCH = 'X' \n",
    "WIKIPEDIA2VEC_HATCH = '.'\n",
    "\n",
    "system_styles = {\n",
    "    'doc2vec_siamese': dict(color='#b2abd2', hatch=DOC2VEC_HATCH),\n",
    "    'wikipedia2vec_siamese': dict(color='#e66101', hatch=WIKIPEDIA2VEC_HATCH),\n",
    "    'smash_paragraph_level': dict(color='#abdda4'),\n",
    "    'smash_sentence_level': dict(color='#fdae61'),\n",
    "    'smash_word_level': dict(color='#2b83ba'),\n",
    "}\n",
    "\n",
    "SMALL_SIZE = 10\n",
    "MEDIUM_SIZE = 12\n",
    "BIGGER_SIZE = 14\n",
    "\n",
    "plt.rc('font', size=SMALL_SIZE)          # controls default text sizes\n",
    "plt.rc('axes', titlesize=SMALL_SIZE)     # fontsize of the axes title\n",
    "plt.rc('axes', labelsize=MEDIUM_SIZE)    # fontsize of the x and y labels\n",
    "plt.rc('xtick', labelsize=SMALL_SIZE+1)    # fontsize of the tick labels\n",
    "plt.rc('ytick', labelsize=SMALL_SIZE+1)    # fontsize of the tick labels\n",
    "plt.rc('legend', fontsize=SMALL_SIZE+1)    # legend fontsize\n",
    "plt.rc('figure', titlesize=BIGGER_SIZE)  # fontsize of the figure title\n",
    "\n",
    "plt.rc('pdf', fonttype=42)\n",
    "plt.rc('ps', fonttype=42)\n",
    "\n",
    "plt.rc('text', usetex=False)\n",
    "plt.rc('font', family='serif')\n",
    "\n",
    "def get_performance_figure(\n",
    "    results,\n",
    "    models,\n",
    "    feature_column,\n",
    "    x_label,\n",
    "    y_label=None,\n",
    "    figsize=(13, 6),\n",
    "    legend_columns_count=3,\n",
    "    buckets_count=5,\n",
    "    save_file_name=None,\n",
    "):\n",
    "    bin_column = f\"{feature_column}_bin\"\n",
    "    bins = pd.qcut(results[feature_column], q=buckets_count)\n",
    "\n",
    "    results[bin_column] = bins\n",
    "    result_by_model = results.groupby([bin_column]).mean()[models]\n",
    "    print(result_by_model)\n",
    "\n",
    "#     fig = plt.figure(figsize=figsize)\n",
    "\n",
    "#     ax = result_by_model.plot(\n",
    "#         kind=\"bar\", ax=fig.gca(), rot=0, width=0.7, alpha=0.9, edgecolor=[\"black\"],\n",
    "#     )\n",
    "\n",
    "#     box = ax.get_position()\n",
    "#     ax.set_position([box.x0, box.y0 + box.height * 0.25, box.width, box.height * 0.75])\n",
    "\n",
    "#     # Formats the bars\n",
    "#     for container in ax.containers:\n",
    "#         container_system = container.get_label()\n",
    "        \n",
    "#         style = system_styles[container_system]\n",
    "#         for patch in container.patches:\n",
    "#             if 'color' in style:\n",
    "#                 patch.set_color(style['color'])\n",
    "#             if 'hatch' in style:\n",
    "#                 patch.set_hatch(style['hatch'])\n",
    "#             if 'linewidth' in style:\n",
    "#                 patch.set_linewidth(style['linewidth'])\n",
    "#             if 'edgecolor' in style:\n",
    "#                 patch.set_edgecolor(style['edgecolor'])\n",
    "#             else:\n",
    "#                 patch.set_edgecolor('black')\n",
    "\n",
    "    \n",
    "#     model_names = [CLEAN_MODEL_NAMES[model] for model in selected_models]\n",
    "\n",
    "#     ax.legend(\n",
    "#         model_names,\n",
    "#         ncol=legend_columns_count,\n",
    "#         loc=\"upper center\",\n",
    "#         fancybox=True,\n",
    "#         shadow=False,\n",
    "#         bbox_to_anchor=(0.5, 1.2),\n",
    "#     )\n",
    "\n",
    "#     # Formats the x label as \"(lower, upper]\"\n",
    "#     ax.set_xticklabels(\n",
    "#         [f\"({int(i.left)}, {int(i.right)}]\" for i in bins.cat.categories]\n",
    "#     )\n",
    "\n",
    "#     y_label = \"NDCG@10\"\n",
    "#     ax.set_xlabel(x_label % len(result_by_model))\n",
    "#     ax.set_ylabel(y_label)\n",
    "    \n",
    "#     if save_file_name:\n",
    "#         save_file_path = f\"./results/figures/{save_file_name}.png\"\n",
    "#         pdf_dpi = 300\n",
    "\n",
    "# #         logger.info(f\"Saved to {save_file_path}\")\n",
    "#         plt.savefig(save_file_path, bbox_inches=\"tight\", dpi=pdf_dpi)\n",
    "\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    smash_word_level  smash_sentence_level  \\\n",
      "sentence_count_bin                                           \n",
      "(0.999, 14.0]                 0.5179                0.5083   \n",
      "(14.0, 32.25]                 0.5440                0.5302   \n",
      "(32.25, 51.0]                 0.5491                0.5382   \n",
      "(51.0, 71.0]                  0.5366                0.5042   \n",
      "(71.0, 101.625]               0.5023                0.4727   \n",
      "(101.625, 149.0]              0.5055                0.4585   \n",
      "(149.0, 256.0]                0.4495                0.4195   \n",
      "(256.0, 1189.0]               0.3709                0.3819   \n",
      "\n",
      "                    smash_paragraph_level  \n",
      "sentence_count_bin                         \n",
      "(0.999, 14.0]                      0.5104  \n",
      "(14.0, 32.25]                      0.5217  \n",
      "(32.25, 51.0]                      0.5592  \n",
      "(51.0, 71.0]                       0.5111  \n",
      "(71.0, 101.625]                    0.5060  \n",
      "(101.625, 149.0]                   0.4785  \n",
      "(149.0, 256.0]                     0.4490  \n",
      "(256.0, 1189.0]                    0.3790  \n"
     ]
    }
   ],
   "source": [
    "selected_models = SMASH_MODELS\n",
    "n_buckets = 8\n",
    "\n",
    "# get_performance_figure(_results, selected_models, WORD_COUNT_COLUMN, \"Text length as word count (%s equal-sized buckets)\", buckets_count=n_buckets, save_file_name=\"best_models_word_count\")\n",
    "get_performance_figure(_results, selected_models, SENTENCE_COUNT_COLUMN, \"Text length as sentence count (%s equal-sized buckets)\", buckets_count=n_buckets, save_file_name=\"smash_sentence_count\")\n",
    "# get_performance_figure(_results, selected_models, PARAGRAPH_COUNT_COLUMN, \"Text length as paragraph count (%s equal-sized buckets)\", buckets_count=n_buckets, save_file_name=\"smash_paragraph_count\")\n",
    "# get_performance_figure(_results, \n",
    "#                        selected_models, \n",
    "#                        IN_LINKS_COLUMN, \n",
    "#                        \"Number of articles with links pointing to the source article (%s equal-sized buckets)\",\n",
    "#                        buckets_count=n_buckets, \n",
    "#                        save_file_name=\"smash_in_links\")\n",
    "\n",
    "\n",
    "# results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.34982935153583616"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(.4746/.3516) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source_article</th>\n",
       "      <th>smash_word_level</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>191</th>\n",
       "      <td>Ireland</td>\n",
       "      <td>0.2470</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    source_article  smash_word_level\n",
       "191        Ireland            0.2470"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_results[_results[\"source_article\"] == \"Ireland\"][[\"source_article\", \"smash_word_level\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "from scipy import stats\n",
    "import random\n",
    "\n",
    "ttest, pval = stats.ttest_rel(_results[\"smash_word_level\"], _results[\"smash_paragraph_level\"])\n",
    "\n",
    "print(pval <= 0.01)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import zip_longest\n",
    "\n",
    "def grouper(iterable, n, fillvalue=None):\n",
    "    \"Collect data into fixed-length chunks or blocks\"\n",
    "    # grouper('ABCDEFG', 3, 'x') --> ABC DEF Gxx\"\n",
    "    args = [iter(iterable)] * n\n",
    "    return list(zip_longest(*args))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_performance_figure_multiple_rows(\n",
    "    results,\n",
    "    models,\n",
    "    feature_column,\n",
    "    x_label,\n",
    "    y_label=None,\n",
    "    figsize=(15, 13),\n",
    "    legend_columns_count=3,\n",
    "    buckets_count=9,\n",
    "    charts_per_row=3,\n",
    "    save_file_name=None,\n",
    "):\n",
    "    bin_column = f\"{feature_column}_bin\"\n",
    "    bins = pd.qcut(results[feature_column], q=buckets_count)\n",
    "\n",
    "#     bins = pd.cut(results[feature_column], bins=[0, 0.05, 0.1, 0.25, 1])\n",
    "\n",
    "    results[bin_column] = bins\n",
    "    result_by_model = results.groupby([bin_column]).mean()[models]\n",
    "\n",
    "    fig = plt.figure(figsize=figsize)\n",
    "    \n",
    "    groups = grouper(range(buckets_count), charts_per_row)\n",
    "    \n",
    "    results_by_group = []\n",
    "    n_rows = len(groups)\n",
    "    for index, group in enumerate(groups):\n",
    "        fig.add_subplot(n_rows, 1, index + 1)\n",
    "        ax = result_by_model.take(group).plot(kind=\"bar\", ax=fig.gca(), rot=0, width=0.7, alpha=0.9, edgecolor=[\"black\"], xlabel=\"\")\n",
    "        ax.get_legend().remove()\n",
    "        \n",
    "        # Formats the x label as \"(lower, upper]\"\n",
    "        if feature_column == MISSING_WORDS_COLUMN:\n",
    "            ax.set_xticklabels([f\"({i.left:0.0%}, {i.right:0.0%}]\" for i in bins.cat.categories[group[0]:(group[-1] + 1)]])\n",
    "        else:\n",
    "            ax.set_xticklabels([f\"({int(i.left)}, {int(i.right)}]\" for i in bins.cat.categories[group[0]:(group[-1] + 1)]])\n",
    "        \n",
    "        y_label = \"NDCG@k (k=5)\"\n",
    "        ax.set_ylabel(y_label)\n",
    "\n",
    "        # Formats the bars\n",
    "        for container in ax.containers:\n",
    "            container_system = container.get_label()\n",
    "\n",
    "            style = system_styles[container_system]\n",
    "            for patch in container.patches:\n",
    "                if 'color' in style:\n",
    "                    patch.set_color(style['color'])\n",
    "                if 'hatch' in style:\n",
    "                    patch.set_hatch(style['hatch'])\n",
    "                if 'linewidth' in style:\n",
    "                    patch.set_linewidth(style['linewidth'])\n",
    "                if 'edgecolor' in style:\n",
    "                    patch.set_edgecolor(style['edgecolor'])\n",
    "                else:\n",
    "                    patch.set_edgecolor('black')\n",
    "                   \n",
    "    ax.set_xlabel(x_label % len(result_by_model))\n",
    "    \n",
    "    top_limits = [axis.get_ylim()[1] for axis in fig.get_axes()]\n",
    "    max_top_limit = max(top_limits)\n",
    "    for axis in fig.get_axes():\n",
    "        axis.set_ylim(top=max_top_limit)\n",
    "\n",
    "    \n",
    "    model_names = [CLEAN_MODEL_NAMES[model] for model in models]\n",
    "\n",
    "    fig.legend(\n",
    "        model_names,\n",
    "        ncol=legend_columns_count,\n",
    "        loc=\"upper center\",\n",
    "        fancybox=True,\n",
    "        shadow=False,\n",
    "        bbox_to_anchor=(0.5, 0.93),\n",
    "    )\n",
    "    \n",
    "    if save_file_name:\n",
    "        save_file_path = f\"./results/figures/{save_file_name}.png\"\n",
    "        pdf_dpi = 300\n",
    "\n",
    "#         logger.info(f\"Saved to {save_file_path}\")\n",
    "        plt.savefig(save_file_path, bbox_inches=\"tight\", dpi=pdf_dpi)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "get_performance_figure_multiple_rows(_results, \n",
    "                                     COMPLETE_MODELS, \n",
    "                                     IN_LINKS_COLUMN, \n",
    "                                     \"Number of articles with links present in the source articles (%s equal-sized buckets)\",\n",
    "                                     buckets_count=20, \n",
    "                                     charts_per_row=4,\n",
    "                                     save_file_name=\"in_links_count\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame([[1, 2], [4, 5], [7, 8], [30, 50]],\n",
    "     index=['cobra', 'cobra', 'viper', 'viper'],\n",
    "     columns=['max_speed', 'shield'])\n",
    "df.reset_index(inplace=True)\n",
    "\n",
    "a = pd.concat({\n",
    "    key: value.reset_index(drop=True) for key, value in df.groupby(\"index\")[\"max_speed\"]\n",
    "}, axis=1)\n",
    "\n",
    "a\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_results.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_results[_results[\"missing_words_percentage\"].isin(_results[\"missing_words_percentage\"].nlargest(5))][[\"source_article\", WIKIPEDIA2VEC_SIAMESE,\n",
    "         DOC2VEC_SIAMESE,\n",
    "        SMASH_WORD_LEVEL, \n",
    "        SMASH_SENTENCE_LEVEL,\n",
    "        SMASH_PARAGRAPH_LEVEL,]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_correlation():\n",
    "    m = [WIKIPEDIA2VEC_SIAMESE,\n",
    "         DOC2VEC_SIAMESE,\n",
    "        SMASH_WORD_LEVEL, \n",
    "        SMASH_SENTENCE_LEVEL,\n",
    "        SMASH_PARAGRAPH_LEVEL,]\n",
    "    \n",
    "    filtered_results = _results\n",
    "    \n",
    "    for model in m:\n",
    "        correlation = round(np.corrcoef(filtered_results[\"missing_words_percentage\"], filtered_results[model])[0, 1],4)\n",
    "        \n",
    "        print(f\"Correlation {CLEAN_MODEL_NAMES[model]}: {correlation}\")\n",
    "        \n",
    "get_correlation()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.4 64-bit ('thesis-davi': conda)",
   "language": "python",
   "name": "python37464bitthesisdaviconda173753154baf4ea5aebc1bb5b4a5349a"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
