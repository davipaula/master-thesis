{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Source: https://github.com/MarvinLSJ/LSTM-siamese\n",
    "- Download spanish embeddings from: https://fasttext.cc/docs/en/pretrained-vectors.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Brief introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Intro:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To do"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to use:\n",
    "\n",
    "Main file is **'siamese.py'**, the configuration file is **'siamese-config.yaml'**.  \n",
    "\n",
    "#### 1. Initialization:\n",
    "    \n",
    "    First run with the original dataset, you should input parameters **'--config siamese-config.yaml'**, while configuring **make_dict** and **data_preprocessing** as **True**, it will make the embedding and preprocess the original dataset, saved them for future usage.  \n",
    "\n",
    "\n",
    "#### 2. Model:\n",
    "\n",
    "**Tuning Parameters**:\n",
    "    \n",
    "    a. Classifier\n",
    "    \n",
    "       fc_dim: classifier fully connected layer size, \n",
    "    \n",
    "    b. Encoder\n",
    "           \n",
    "       hidden_size: lstm hidden size\n",
    "       num_layers: lstm layer \n",
    "       bidirectional: bidirectional lstm can get more info\n",
    "       dropout: avoid overfitting\n",
    "        \n",
    "    c. Embedding\n",
    "           \n",
    "       embedding_freeze: Set it to false, then the embedding will participate backpropogation. Not so good from my experience, especially small training dataset.\n",
    "    \n",
    "    d. Other\n",
    "    \n",
    "**Structure**:\n",
    "    \n",
    "    a. Classifier\n",
    "        \n",
    "        fc layers, non-linear fc layers(add ReLU)\n",
    "    \n",
    "    b. Encoder \n",
    "        \n",
    "        Features generating method, current method is (v1, v2, abs(v1-v2), v1*v2), more features with different vector distance measurement?\n",
    "        \n",
    "    \n",
    "#### 3. Training\n",
    "\n",
    "    a. Optimizer\n",
    "    \n",
    "       Default SGD, lots of optimizer in torch.\n",
    "\n",
    "    b. Learning rate\n",
    "    \n",
    "       It should be small enough to avoid oscillation. Furthur exploration can be dynamic lr clipping.\n",
    "        \n",
    "    c. Other"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To do list:\n",
    "\n",
    "    1. Early stopping\n",
    "    \n",
    "    2. Dynamic learning rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    'experiment_name': 'siamese-baseline',\n",
    "    'task': 'train',\n",
    "    'make_dict': False,  # use pre-processed embeddings (from pickle)\n",
    "    'data_preprocessing': True,\n",
    "\n",
    "    'ckpt_dir': './siamese/ckpt/',\n",
    "\n",
    "    'training':{\n",
    "        'num_epochs': 2, #20,   # decreased for debugging\n",
    "        'learning_rate': 0.01,\n",
    "        'optimizer': 'sgd'\n",
    "    },\n",
    "    \n",
    "    'embedding':{\n",
    "        'full_embedding_path': '/Users/dnascimentodepau/Documents/python/thesis/thesis-davi/input/lstm-siamese/wiki.es.vec',  # fastText embeddings (change path!)\n",
    "        'cur_embedding_path': 'input/embedding.pkl',\n",
    "    },\n",
    "        \n",
    "    'model':{\n",
    "        'fc_dim': 100,\n",
    "        'name': 'siamese',\n",
    "        'embed_size': 300,\n",
    "        'batch_size': 1,\n",
    "        'embedding_freeze': False,\n",
    "        'encoder':{\n",
    "            'hidden_size': 150,\n",
    "            'num_layers': 1,\n",
    "            'bidirectional': False,\n",
    "            'dropout': 0.0,\n",
    "        },  \n",
    "    },   \n",
    "    \n",
    "    'result':{\n",
    "        'filename':'result.txt',\n",
    "        'filepath':'res/',\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#coding=utf-8\n",
    "import os\n",
    "import yaml\n",
    "import argparse\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "# utils\n",
    "from siamese.utils import get_embedding, load_embed, save_embed, data_preprocessing\n",
    "# data\n",
    "from siamese.data import myDS, mytestDS\n",
    "# model\n",
    "from siamese.model import Siamese_lstm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Preprocess the original data:** \n",
    "\n",
    "    1. Remove spanish punctuations, convert unique spanish uppercase letters into lower case since .lower() can not be applied to spanish uppercase letters.      \n",
    "        E.g. Remove ¡,¿; Convert Á to á, Ó to ó, Ú to ú, É to é, Í to í.  \n",
    "        \n",
    "    2. Remove spanish stopwords.  \n",
    "    \n",
    "    3. After stopwords removal, there are some empty sentences, drop those sentences pairs.\n",
    "\n",
    "Details in **utils.py** function: **data_preprocessing**  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Data Preprocessing \"\"\"\n",
    "\n",
    "if config['data_preprocessing']:\n",
    "    print('Pre-processing Original Data ...')\n",
    "    data_preprocessing()\n",
    "    print('Data Pre-processing Done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Spliting and Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Split train_data into train and valid, load them into myDS**:\n",
    "    \n",
    "    1. Split train_data into train, valid by 0.8\n",
    "    \n",
    "    2. Load them into myDS, with vocabulary of all train_data and test_data.\n",
    "    \n",
    "Details in **data.py** class **myDS** and **Vocab**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Read Data \"\"\"\n",
    "\n",
    "train_data = pd.read_csv('input/cleaned_train.csv')\n",
    "\n",
    "train_data = train_data[:500] # decreased for debugging\n",
    "\n",
    "test_data = pd.read_csv('input/cleaned_test.csv')\n",
    "\n",
    "# split dataset\n",
    "msk = np.random.rand(len(train_data)) < 0.8\n",
    "train = train_data[msk]\n",
    "valid = train_data[~msk]\n",
    "all_sents = train_data['s1'].tolist() + train_data['s2'].tolist() + test_data['s1'].tolist() + test_data['s2'].tolist()\n",
    "\n",
    "# dataset\n",
    "trainDS = myDS(train, all_sents)\n",
    "validDS = myDS(valid, all_sents)\n",
    "\n",
    "print('Data size:',train_data.shape[0], test_data.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Embedding\n",
    "\n",
    "**Generating or loading saved embeddings**\n",
    "\n",
    "    1. Retrieving known words' embeddings in vocabulary and initialize unknown words' embeddings including <sos>,<eos>,<unk>,<pad>\n",
    "    \n",
    "    2. Loading existing embeddings if there is one.\n",
    "    \n",
    "    3. Get embedding weight_matrix and convert to nn.Embeddings, add to config.\n",
    "    \n",
    "Details in **utils.py** function **get_embedding**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Get Embedding \"\"\"\n",
    "\n",
    "full_embed_path = config['embedding']['full_embedding_path']\n",
    "cur_embed_path = config['embedding']['cur_embedding_path']\n",
    "\n",
    "if os.path.exists(cur_embed_path) and not config['make_dict']:\n",
    "    embed_dict = load_embed(cur_embed_path)\n",
    "    print('Loaded existing embedding.')\n",
    "else:\n",
    "    print('Making embedding...')\n",
    "    embed_dict = get_embedding(trainDS.vocab._id2word, full_embed_path)\n",
    "    save_embed(embed_dict,cur_embed_path)\n",
    "    print('Saved generated embedding.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make embedding weight_matrix in the same order of embed_dict, which can be refered to words embeddings when the sentences are converted into ids. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(embed_dict)\n",
    "# initialize nn embedding\n",
    "embedding = nn.Embedding(vocab_size, config['model']['embed_size'])\n",
    "embed_list = []\n",
    "for word in trainDS.vocab._id2word:\n",
    "    embed_list.append(embed_dict[word])\n",
    "weight_matrix = np.array(embed_list)\n",
    "# pass weights to nn embedding\n",
    "embedding.weight = nn.Parameter(torch.from_numpy(weight_matrix).type(torch.FloatTensor), requires_grad = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add to config."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embedding\n",
    "config['embedding_matrix'] = embedding\n",
    "config['vocab_size'] = len(embed_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from siamese.model import LSTMEncoder\n",
    "config\n",
    "\n",
    "#encoder = LSTMEncoder(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model\n",
    "siamese = Siamese_lstm(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loss function\n",
    "Considering the amounts of Pos and Neg samples are roughly 1:3, thus conpensate the pos samples on loss weights, set the loss weight as 1:3 (Neg:Pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss func\n",
    "loss_weights = Variable(torch.FloatTensor([1, 3]))\n",
    "if torch.cuda.is_available():\n",
    "    loss_weights = loss_weights.cuda()\n",
    "criterion = torch.nn.CrossEntropyLoss(loss_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optimizer\n",
    "\n",
    "Using SGD as default, learning rate 0.01.  \n",
    "\n",
    "Possible options: adam, adadelta, rmsprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer\n",
    "learning_rate = config['training']['learning_rate']\n",
    "if config['training']['optimizer'] == 'sgd':\n",
    "    optimizer = torch.optim.SGD(filter(lambda x: x.requires_grad, siamese.parameters()), lr=learning_rate)\n",
    "elif config['training']['optimizer'] == 'adam':\n",
    "    optimizer = torch.optim.Adam(filter(lambda x: x.requires_grad, siamese.parameters()), lr=learning_rate)\n",
    "elif config['training']['optimizer'] == 'adadelta':\n",
    "    optimizer = torch.optim.Adadelta(filter(lambda x: x.requires_grad, siamese.parameters()), lr=learning_rate)\n",
    "elif config['training']['optimizer'] == 'rmsprop':\n",
    "    optimizer = torch.optim.RMSprop(filter(lambda x: x.requires_grad, siamese.parameters()), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Log info to be printed during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# log info\n",
    "train_log_string = '%s :: Epoch %i :: Iter %i / %i :: train loss: %0.4f'\n",
    "valid_log_string = '%s :: Epoch %i :: valid loss: %0.4f\\n'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Restoring saved model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Restore saved model (if one exists).\n",
    "ckpt_path = os.path.join(config['ckpt_dir'], config['experiment_name']+'.pt')\n",
    "\n",
    "if os.path.exists(ckpt_path):\n",
    "    print('Loading checkpoint: %s' % ckpt_path)\n",
    "    ckpt = torch.load(ckpt_path)\n",
    "    epoch = ckpt['epoch']\n",
    "    siamese.load_state_dict(ckpt['siamese'])\n",
    "    optimizer.load_state_dict(ckpt['optimizer'])\n",
    "else:\n",
    "    epoch = 0\n",
    "    print('Fresh start!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\"\"\" Train \"\"\"\n",
    "\n",
    "if config['task'] == 'train':\n",
    "    \n",
    "    # save every epoch for visualization\n",
    "    train_loss_record = []\n",
    "    valid_loss_record = []\n",
    "    best_record = 10.0\n",
    "    \n",
    "    # training\n",
    "    print('Experiment:{}\\n'.format(config['experiment_name']))\n",
    "\n",
    "    \n",
    "    while epoch < config['training']['num_epochs']:\n",
    "        \n",
    "        print('Start Epoch{} Training...'.format(epoch))\n",
    "        \n",
    "        # loss\n",
    "        train_loss = []\n",
    "        train_loss_sum = []\n",
    "        # dataloader\n",
    "        train_dataloader = DataLoader(dataset=trainDS, shuffle=True, num_workers=2, batch_size=1)\n",
    "\n",
    "        for idx, data in enumerate(train_dataloader, 0):\n",
    "\n",
    "            # get data\n",
    "            s1, s2, label = data\n",
    "\n",
    "            # clear gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # input\n",
    "            output = siamese(s1, s2)\n",
    "            output = output.squeeze(0)\n",
    "\n",
    "            # loss backward\n",
    "            loss = criterion(output, Variable(label))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss.append(loss.data.cpu())\n",
    "            train_loss_sum.append(loss.data.cpu())\n",
    "            \n",
    "            # Every once and a while check on the loss\n",
    "            if ((idx + 1) % 5000) == 0:\n",
    "                print(train_log_string % (datetime.now(), epoch, idx + 1, len(train), np.mean(train_loss)))\n",
    "                train_loss = []\n",
    "        \n",
    "        # Record at every epoch\n",
    "        print( 'Train Loss at epoch{}: {}\\n'.format(epoch, np.mean(train_loss_sum)))\n",
    "        train_loss_record.append(np.mean(train_loss_sum))\n",
    "\n",
    "        # Valid\n",
    "        print('Epoch{} Validating...'.format(epoch))\n",
    "\n",
    "        # loss\n",
    "        valid_loss = []\n",
    "        # dataloader\n",
    "        valid_dataloader = DataLoader(dataset=validDS, shuffle=True, num_workers=2, batch_size=1)\n",
    "\n",
    "        for idx, data in enumerate(valid_dataloader, 0):\n",
    "            # get data\n",
    "            s1, s2, label = data\n",
    "\n",
    "            # input\n",
    "            output = siamese(s1, s2)\n",
    "            output = output.squeeze(0)\n",
    "\n",
    "            # loss\n",
    "            loss = criterion(output, Variable(label))\n",
    "            valid_loss.append(loss.data.cpu())\n",
    "\n",
    "        print(valid_log_string % (datetime.now(), epoch, np.mean(valid_loss)))\n",
    "        # Record\n",
    "        valid_loss_record.append(np.mean(valid_loss))\n",
    "\n",
    "        epoch += 1\n",
    "\n",
    "        # Keep track of best record\n",
    "        if np.mean(valid_loss) < best_record:\n",
    "            best_record = np.mean(valid_loss)\n",
    "            # save the best model\n",
    "            state_dict = {\n",
    "                'epoch': epoch,\n",
    "                'siamese': siamese.state_dict(),\n",
    "                'optimizer': optimizer.state_dict(),\n",
    "            }\n",
    "            torch.save(state_dict, ckpt_path)\n",
    "            print( 'Model saved!\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_loss_record)\n",
    "plt.plot(valid_loss_record)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test set Loading  \n",
    "\n",
    "    1. Test set is a little bit different since it has no labels, load test_data into mytestDS.\n",
    "    \n",
    "    2. No shuffling at dataloader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Inference \"\"\"\n",
    "# if config['taks'] == 'inference':\n",
    "testDS = mytestDS(test_data, all_sents)\n",
    "# Do not shuffle here\n",
    "test_dataloader = DataLoader(dataset=testDS, num_workers=2, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = []\n",
    "for idx, data in enumerate(test_dataloader, 0):\n",
    "\n",
    "    # get data\n",
    "    s1, s2 = data\n",
    "\n",
    "    # input\n",
    "    output = siamese(s1,s2)\n",
    "    output = output.squeeze(0)\n",
    "\n",
    "    # feed output into softmax to get prob prediction\n",
    "    sm = nn.Softmax(dim=1)\n",
    "    res = sm(output.data)[:,1]\n",
    "    result += res.data.tolist()\n",
    "\n",
    "result = pd.DataFrame(result)\n",
    "print('Inference Done.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Write down the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_path = os.path.join(config['result']['filepath'], config['result']['filename'])\n",
    "result.to_csv(res_path,header=False,index=False)\n",
    "print('Result has writtn to', res_path, ', Good Luck!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
